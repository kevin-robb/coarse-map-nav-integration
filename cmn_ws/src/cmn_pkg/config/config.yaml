# Parameters for the project

# Rostopics used throughout the project.
topics:
  measurements: "/locobot/camera/color/image_raw"
  observations: "/observation"
  occ_map: "/map/occ"
  raw_map: "/map/raw"
  localization: "/state_est"
  goal: "/goal"
  commands: "/locobot/mobile_base/commands/velocity"
  planned_path: "/planned_path"

map:
  fname: "building1.png" # Name of the map image file to use. Image should be in cmn_pkg/config/maps/ directory.
  downscale_ratio: 0.2 # Resize map to lower resolution. This helps A* to run faster. New side lengths will be original*downscale_ratio.
  resolution: 0.01 # Assumed map resolution, in meters/pixel. This will later be estimated as part of the filter to allow a map with unknown scale to be used.
  occ_thresh: 200 # Integer in range [0,255]. All cells with grayscale value < this will be considered occupied.
  obstacle_balloon_radius: 2 # Number of cells to expand all occupied cells by. Expansion happens in a square, treating diagonal and adjacent distances as equivalent.

# Config params defining how the observation output from the perception model relates to the robot.
# These should be known, since they are determined by the observation "ground truth" outputs used to train the model in the first place.
observation:
  resolution: 0.1 # Image resolution, in meters/pixel.
  height: 5 # in pixels. forward distance ahead of vehicle.
  width: 5 # in pixels. lateral span.
  # Coordinates on the observation image frame the vehicle's POV is.
  veh_horz_pos_ratio: 0.5 # relative position from left (0) to right (1). Use 0.5 to center it.
  veh_vert_pos_ratio: 0.0 # relative position from bottom (0) to top (1). Use 0.5 to center it.

perception_node_dt: 0.1 # timer period of perception_node's main update loop (seconds). NOTE later we can characterize the runtime of the model to optimize this value.

particle_filter:
  num_particles: 10 # Number of particles to track.
  state_size: 3 # Number of variables in the state. (i.e., (x,y,yaw)).
  random_sampling_rate: 0.1 # Proportion in range [0, 1] of population that will be sampled randomly instead of from the previous generation.

path_planning:
  do_path_planning: false # if true, use A* to compute a full path to the goal point, then use pure pursuit to choose a motion command. if false, navigate directly towards goal point.

# Configs for running simulator node for basic testing.
simulator:
  ground_truth_map:
    fname: "igvc1.png" # Name of the map image file to use. Image should be in cmn_pkg/config/maps/ directory.
    resolution: 0.01 # Map resolution in meters/pixel.
    occ_thresh: 200 # Integer in range [0,255]. All cells with grayscale value < this will be considered occupied.
    obstacle_balloon_radius: 2 # Number of cells to expand all occupied cells by. Expansion happens in a square, treating diagonal and adjacent distances as equivalent.
  ground_truth_pose:
    veh_x_initial: 0.0 # meters from left.
    veh_y_initial: 0.0 # meters from bottom.
    veh_yaw_initial: 0.0 # radians. 0 = facing right, increases CCW.

constraints:
  fwd: 0.1 # commanded motion in a single iteration must be between 0 and this amount (meters).
  ang: 0.0546 # commanded angular motion in a single iteration has its magnitude limited by this amount (radians).


test:
  run_debug_mode: false # Run the test mode, which will display images and print text that are normally skipped.
  save_data_for_training: false # Save measurements, true odom pose on map, and "observation" pulled from the map, all with timestamps & frame numbers to associate them. This data can be used to retrain the ML model, and also allows re-running the project to evaluate the model & localization to compare to ground truth. May also need to save commands or relative motions depending on how the localization is implemented. NOTE could use rosbags for the replaying aspect, but for training the model we still need a way to get associated measurements + "observations".
